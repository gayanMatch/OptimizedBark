stage: semantic_train
start_path: '..'

common:
  trainer:
    accelerator: "gpu"
#    strategy: 'deepspeed'
    precision: 16 # 32
    val_check_interval: 0.2
    limit_val_batches: 0.1

  ckpt:
    mode: "min"
    monitor: "val_loss"
    save_top_k: 3
#    every_n_epochs: 10
    every_n_train_steps: 10000
    save_weights_only: false


semantic:
  dataloader:
    batch_size: 64
    shuffle: True
    num_workers: 1
    persistent_workers: True
    pin_memory: True

  ############## model params ##############
  model:
    bias: False
    block_size: 1024
    dropout: 0.1
    input_vocab_size: 129600
    n_embd: 768
    n_head: 24
    n_layer: 6
    output_vocab_size: 10048

  ############## optim params ##############
  optim:
    lr: 1e-4
    min_lr: 2e-5
    weight_decay: 1e-3
    warmup_iters: 1000
    max_iters: 100000 # depends on your GPUs.
    lr_strategy: 'cosine'
    gradient_clip: 1